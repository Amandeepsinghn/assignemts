{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc7c3765-f6e0-40df-bfc9-ac8c09965b4d",
   "metadata": {},
   "source": [
    "# PART 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09708f2-b92d-4a15-a1be-c4e8aaaa1810",
   "metadata": {},
   "source": [
    "### ANS 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a654d9-c506-4006-b125-ade5f677b19c",
   "metadata": {},
   "source": [
    "optimization algorithms are used to find the optimal wegihts which fits best in the model and there are many types of optimizational algorithms:\n",
    "1) Gradient descent \n",
    "2) stochastic gradient descent \n",
    "3) mini-batch gradient descent \n",
    "4) Adam optimization \n",
    "***\n",
    "they are necessary as to atain global minima we need to find right. it helps in avoiding local minima and finding the right global minima. optimization are essentaly to train model faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207491b6-4e52-44ae-a6ea-e64e5c0f2557",
   "metadata": {},
   "source": [
    "### ANS 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940e5a19-ab31-4fd4-92c7-76c69675240c",
   "metadata": {},
   "source": [
    "gradient descent is a technique in machine learning which is associated with loss function and weights. we calculate the gradient and is used to update the weights. Varaints of gradient descent are:\n",
    "1) stochastic gradient descent: instead of using whole dataset we calcuate loss function for each row wise data.\n",
    "2) mini_batch gradient descent: we updates weights on the basis of batch size.In this descent we take number of bathces to divide the data.\n",
    "3) Adaptive gradient: it adaps learning rates for each parameter based on historical gradient. \n",
    "4) Rmsprop: to solve the problem of adaptive gradient which almost makes the new weight updation same as old weight. it changes the formula of adaptive gradient but also consider historical gradient. as it helps in speed up the converging speed.\n",
    "5) Adam: it is the most commly used optizmation techniqiue it combines the property of rmsprop and gradient descent with momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b752fae3-95eb-4a7c-8dea-8851f534a2cd",
   "metadata": {},
   "source": [
    "### ANS 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3e8bb1-6b5c-4214-9bfd-232ce24d5d40",
   "metadata": {},
   "source": [
    "Traditional gradient descent optimizaion methods have few problems like:\n",
    "1) it has usually slow convernge speed.\n",
    "2) it sometimes get stuck in the local minima.\n",
    "3) it gradient is affected by the noises.\n",
    "4) traditional gradient takes whole dataset. \n",
    "5) it is not reliable as new gradient descent optimzation. \n",
    "****\n",
    "modern optimization addresses these problems. adam can be considered as modern opitmzaiton as it combines the property of rmsprop and gradient descent with momentum to solve the above probelms.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5e49bf-9ee1-4f6f-921c-36928e2af12c",
   "metadata": {},
   "source": [
    "### ANS 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df434c69-b031-486e-96de-19bc8be9790f",
   "metadata": {},
   "source": [
    "momentum is used as some gradient descent technique calculate loss function for each data entry and it also take otulier which disrupt the gradient to normalize the outlier effect on gradient we uses momentum to mitigate the problem. learning rate is the convergence speed taken by gradient descent to update the weights rmsprop is the most sufficient type of gradient descent for learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c81d6c-a8c1-4c31-9b96-2ee8bba12323",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd47bc1b-fa9d-4431-b939-ab5f6df5fdb6",
   "metadata": {},
   "source": [
    "### ANS 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed6bcf2-c15b-4ad2-82cc-371fc420f866",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent(SGD): it is gradient descent technique which calculate loss function for each data and update weights for each data entry. \n",
    "Advatages of sgd are:\n",
    "1) it has faster convergence.\n",
    "2) it is more memory effecient \n",
    "3) it is more adpatable as it adapts for every data. \n",
    "**\n",
    "limitation of sgd:\n",
    "1) it also updates weights on outlier also which can make the convergence less stable.\n",
    "2) tunning learning rate is crucial wrong leraning rate can impact the model in bad way. \n",
    "3) it have no momentum \n",
    "4) it is not graunted it will converge to global minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53fe324-5a56-4ece-a47d-615cb3652235",
   "metadata": {},
   "source": [
    "### ANS 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e685ccb-8ee6-45c7-9a60-95982addbf80",
   "metadata": {},
   "source": [
    "Adam optimizer combines the property of both rmsprop and gradient descendent with momentum which solve the tradational gradient problem.\n",
    "Benefits of adam:\n",
    "1) it has mometum \n",
    "2) it has very good covernege speed \n",
    "3) it negelect the outlier effect \n",
    "4) it uses mini batches for weight updation. \n",
    "***\n",
    "drawbacks of adam: \n",
    "1) slecting right hyperparamter can be difficult.\n",
    "2) it is computational expensive "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba7b1e-7163-439f-a01c-d4915a285acc",
   "metadata": {},
   "source": [
    "### ANS 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a18d164-623a-4a03-b76e-4e066cff13e5",
   "metadata": {},
   "source": [
    "Rmsprop optimizer sloves the problem adaptive gradient.As adaptive gradient makes new weight updation almost similar to the old weights. to slove this problem it adds the history gradient to give more emphasis on history gradient to makes the dynamic learning possible. \n",
    "***\n",
    "differnce between adam and rmseprop:\n",
    "1) adam is more realiabe, rmsprop is less realible.\n",
    "2) adam has inherit the property of both meometum and rmseprop but rmspe prop do no inherit their property \n",
    "3) adam is widely used and rmse prop is less widely used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe34dc95-5179-4c90-bbc1-3be62e309943",
   "metadata": {},
   "source": [
    "# PART 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3158594f-7e11-4ac4-8d02-223a94e53fda",
   "metadata": {},
   "source": [
    "### ANS 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b19207-9fcf-44c5-9081-fdc761010d3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d53f3cf7-38be-4368-8c57-8d33effcc427",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\amand\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import time \n",
    "%load_ext tensorboard\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b97f9507-36ad-4f3d-8ea0-c7c08843c517",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(X_train_full,y_train_full),(X_test,y_test)=tf.keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full=X_train_full/255.0\n",
    "X_test=X_test/255.0\n",
    "X_valid,X_train=X_train_full[:5000],X_train_full[5000:]\n",
    "y_valid,y_train=y_train_full[:5000],y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3474a159-a747-450d-a207-185b1f79f015",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\amand\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LAYERS=[tf.keras.layers.Flatten(input_shape=[28,28]),\n",
    "        tf.keras.layers.Dense(300,activation='relu',kernel_initializer='he_normal'),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "        tf.keras.layers.Dense(100,activation='relu',kernel_initializer='he_normal'),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "        tf.keras.layers.Dense(10,activation='softmax')\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4fc1cc8-b436-4fc4-8440-afadb5043740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=tf.keras.models.Sequential(LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00014ec2-8884-472e-ab7c-3421c87f0cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.SGD(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdd1d687-16d6-4713-a594-f1dd0439ec63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 - 6s - loss: 0.3176 - accuracy: 0.8869 - val_loss: 0.3240 - val_accuracy: 0.8864 - 6s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "1719/1719 - 6s - loss: 0.3107 - accuracy: 0.8890 - val_loss: 0.3395 - val_accuracy: 0.8772 - 6s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "1719/1719 - 6s - loss: 0.3028 - accuracy: 0.8919 - val_loss: 0.3232 - val_accuracy: 0.8864 - 6s/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "1719/1719 - 6s - loss: 0.2957 - accuracy: 0.8939 - val_loss: 0.3174 - val_accuracy: 0.8848 - 6s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "1719/1719 - 6s - loss: 0.2892 - accuracy: 0.8970 - val_loss: 0.3186 - val_accuracy: 0.8878 - 6s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "1719/1719 - 6s - loss: 0.2843 - accuracy: 0.8986 - val_loss: 0.3165 - val_accuracy: 0.8900 - 6s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "1719/1719 - 6s - loss: 0.2779 - accuracy: 0.8995 - val_loss: 0.3016 - val_accuracy: 0.8942 - 6s/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "1719/1719 - 6s - loss: 0.2718 - accuracy: 0.9016 - val_loss: 0.3080 - val_accuracy: 0.8908 - 6s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "1719/1719 - 6s - loss: 0.2667 - accuracy: 0.9034 - val_loss: 0.3063 - val_accuracy: 0.8892 - 6s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "1719/1719 - 6s - loss: 0.2612 - accuracy: 0.9070 - val_loss: 0.3044 - val_accuracy: 0.8896 - 6s/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,y_train,epochs=10,validation_data=(X_valid,y_valid), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cd9e79-7e35-49f7-9862-87aed7c6a5e8",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94b55fd8-3420-4208-bb03-6f4ede996d54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37063fc7-7526-445c-94b5-103ff1d7f84a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 - 9s - loss: 0.3870 - accuracy: 0.8590 - val_loss: 0.3581 - val_accuracy: 0.8690 - 9s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "1719/1719 - 7s - loss: 0.3386 - accuracy: 0.8756 - val_loss: 0.3316 - val_accuracy: 0.8810 - 7s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "1719/1719 - 7s - loss: 0.3132 - accuracy: 0.8839 - val_loss: 0.3283 - val_accuracy: 0.8812 - 7s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "1719/1719 - 7s - loss: 0.2935 - accuracy: 0.8919 - val_loss: 0.3332 - val_accuracy: 0.8748 - 7s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "1719/1719 - 7s - loss: 0.2766 - accuracy: 0.8958 - val_loss: 0.3420 - val_accuracy: 0.8820 - 7s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "1719/1719 - 7s - loss: 0.2642 - accuracy: 0.9014 - val_loss: 0.3027 - val_accuracy: 0.8912 - 7s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "1719/1719 - 7s - loss: 0.2551 - accuracy: 0.9037 - val_loss: 0.3031 - val_accuracy: 0.8910 - 7s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "1719/1719 - 7s - loss: 0.2426 - accuracy: 0.9086 - val_loss: 0.3177 - val_accuracy: 0.8884 - 7s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "1719/1719 - 7s - loss: 0.2310 - accuracy: 0.9131 - val_loss: 0.3288 - val_accuracy: 0.8878 - 7s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "1719/1719 - 7s - loss: 0.2274 - accuracy: 0.9137 - val_loss: 0.3187 - val_accuracy: 0.8884 - 7s/epoch - 4ms/step\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,y_train,epochs=10,validation_data=(X_valid,y_valid), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2e8ad0-c4f3-4ee2-b132-b97883b006d3",
   "metadata": {},
   "source": [
    "### RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e7be496-b9d1-4687-8d8b-5ca9e726744d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e545bd5-e18a-41b1-855f-c345c7ec1eef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 - 8s - loss: 0.2174 - accuracy: 0.9185 - val_loss: 0.3932 - val_accuracy: 0.8796 - 8s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "1719/1719 - 7s - loss: 0.2230 - accuracy: 0.9177 - val_loss: 0.3766 - val_accuracy: 0.8924 - 7s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "1719/1719 - 7s - loss: 0.2272 - accuracy: 0.9175 - val_loss: 0.3906 - val_accuracy: 0.8948 - 7s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "1719/1719 - 6s - loss: 0.2290 - accuracy: 0.9185 - val_loss: 0.4502 - val_accuracy: 0.8828 - 6s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "1719/1719 - 7s - loss: 0.2328 - accuracy: 0.9186 - val_loss: 0.4252 - val_accuracy: 0.8854 - 7s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "1719/1719 - 7s - loss: 0.2360 - accuracy: 0.9170 - val_loss: 0.4778 - val_accuracy: 0.8892 - 7s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "1719/1719 - 7s - loss: 0.2345 - accuracy: 0.9190 - val_loss: 0.5172 - val_accuracy: 0.8794 - 7s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "1719/1719 - 7s - loss: 0.2339 - accuracy: 0.9194 - val_loss: 0.5351 - val_accuracy: 0.8882 - 7s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "1719/1719 - 7s - loss: 0.2328 - accuracy: 0.9202 - val_loss: 0.5384 - val_accuracy: 0.8866 - 7s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "1719/1719 - 7s - loss: 0.2310 - accuracy: 0.9205 - val_loss: 0.6008 - val_accuracy: 0.8790 - 7s/epoch - 4ms/step\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,y_train,epochs=10,validation_data=(X_valid,y_valid), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635e69c5-8199-4054-b755-12b5a4cde057",
   "metadata": {},
   "source": [
    "### ANS 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6df9a84-b387-4a0d-8921-06be7026e8c8",
   "metadata": {},
   "source": [
    "1) gradient descent= it computationaly expensive and have less convergence speed.\n",
    "2) sgd= it have more convergence speed but is sensitive to outlier.\n",
    "3) mini batch= it have less convergence speed and may not able to distinguish between local and global minima \n",
    "4) adaptive gradient: At one point it is not able to update the weight correctly.\n",
    "5) rmseprop: it solve the problem of adaptive gradient. \n",
    "6) adam: it combines the property of both rmseprop and gradient desecent with momentum "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
