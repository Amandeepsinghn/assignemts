{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "520a668c",
   "metadata": {},
   "source": [
    "### ANS 1\n",
    "boositng is a type of ensemble technique used in machine learning and there are different types of different algorithms in boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e41bbd9",
   "metadata": {},
   "source": [
    "### ANS 2\n",
    "advantages:\n",
    "1) it is can lead to better accuracy.\n",
    "2) it can reduce overfitting.\n",
    "3) it is robust to noise if tuned properly.\n",
    "4) wegihted average is calculated for the final outcome which lead to beeter accuaracy.\n",
    "5) these algorithim provide insights into feature importance.\n",
    "****\n",
    "Limitations:\n",
    "1) too much computational expensive.\n",
    "2) complexicity it can be complex to tune hyperparamter.\n",
    "3) it can cause problem if data is imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d181d2d",
   "metadata": {},
   "source": [
    "### ANS 3\n",
    "In boosting a model is squentally trained and each base learner is know as weak learne. Whole concept is that if one base learner prediction is wrong than data point should be passed to the next base learner and in the end output of whole model is the weighted average of all base learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1236551f",
   "metadata": {},
   "source": [
    "### ANS 4\n",
    "there are three types of boosting algorithim:\n",
    "1) Adaboost alogrithim (classification)\n",
    "2) gradient algorithim(classification and regression)\n",
    "3) Xg algorithim(classification and regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903db691",
   "metadata": {},
   "source": [
    "### ANS 5\n",
    "common paramter:\n",
    "1) number of estimators: number of weak learner(base learner)\n",
    "2) learning rate: for lower learning rate it require more weak learner but can lead to better generalization.higher learning speed up learning process but may result in overfitting.\n",
    "3) base learner parameter: specific paramter for the base learner.\n",
    "4) subsample ratio: it determines the fraction of training data to be used for fitting the each base learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde9d383",
   "metadata": {},
   "source": [
    "### ANS 6\n",
    "1) each sample in the dataset is assigned equal weight.first weak learner is trained on entire dataset \n",
    "2) after weak learner is trained it identifies the misclassified samples.\n",
    "3) next weak learner is then trained modified dataset with more focus one previously misclassified samples due to increased weight.\n",
    "4) each weak learner makes itw own prediciton based on the data it was trained on. these prediction are combined with the  more accurate models having more influence in the final prediction.\n",
    "5) final prediction is made by aggeragating the output from all weak learner.\n",
    "6) boosting algorithim continues the preocess for a defined number of rounds. at each round algorithim pays more attention to previous misclassified points.\n",
    "***\n",
    "this iterative process, where each subsequent weak learner correct the errors made by its predeceossors,leads to the creation of a strong learner that genrally outperform any weak learner.this combined model can generalize well making strong model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cef8a0",
   "metadata": {},
   "source": [
    "### ANS 7\n",
    "In adaboost instead of decision tree decision tree stump is created based on the enropy or gini and information gain best dt stamp is selected. \n",
    "1) first we make tree stamp based on the information best dt stamp is selected.\n",
    "2) then we calculate the similar weight.\n",
    "3) based on best dt stamp we clacuate the sum of total error. \n",
    "4) based on error we calculate the updated weights\n",
    "5) to make weight between 0 and 1 we normalize weight.\n",
    "6) after normalizing we assign bin assignment(for misclassifed point interval is bigger as compared to other) .\n",
    "7) for the next dt stamp we select value between 0 and 1 and various point will be in missclasifed bin which will be used in next stamp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4d5d4d",
   "metadata": {},
   "source": [
    "### ANS 8\n",
    "exponential loss fucntion is used in adaboost. it is used to calculate the errors in classification task. it assigns higher weights to misclassified instances which helps in boositng tehcnique to train on misclassified point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac7024b",
   "metadata": {},
   "source": [
    "### ANS 9\n",
    "in adaboost we make bins and random points are selected between 0 and 1 for the next dt stamp most point falls under the bin of missclassifed point of previous point which helps us in updating the weights on misclassified samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87eeff9",
   "metadata": {},
   "source": [
    "### ANS 10\n",
    "estimator means weak learner increasing the number of estimator make model stronger :\n",
    "1) it leads to better accuracy \n",
    "2) a very large number of estimator leads to overfitting \\.\n",
    "3) after a certain number of iteration model perform stabalize and additional learner might not improve the model accuracy.\n",
    "4) it helps in generalizing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc1cd91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
