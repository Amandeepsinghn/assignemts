{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c712e8c",
   "metadata": {},
   "source": [
    "### ANS 1\n",
    "In bagging multiple models are trained parllely and each give random feautre and random boostrap dataset because of the less feature given each model leaf node is usally found in upper level which helps in reducing overfitting of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9acd6a",
   "metadata": {},
   "source": [
    "### ANS 4\n",
    "yes bagging can be used for both classification and regression task. In classification we find the most repeated ouput of multiple models to find the outcome, but in regression task we calcualte mean of output different models to find the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4e5d8c",
   "metadata": {},
   "source": [
    "### ANS 6\n",
    "lets suppose we have to find the average of student score.\n",
    "1) dataset= maths test question \n",
    "2) n data point or maths question get selected randomly making essemble dataset. \n",
    "3) here base learner are our student lests suppose there are 5 base learner. each given essemble dataset.\n",
    "4) base learner scored different score. \n",
    "5) we calculate the mean of base learner \n",
    "6) mean value is our average score.\n",
    "****\n",
    "this is real world example of bagging regression task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2e2ca3",
   "metadata": {},
   "source": [
    "### ANS 3 \n",
    "1) low bias= it means model has captured the complexity of the model of the training dataset.\n",
    "2) high bias= it means model is very simple and  may underfit the data.\n",
    "3) low variance= it generally perform better with the new unseen data.\n",
    "4) high variance= means it not able to adapt to new unseen data.\n",
    "*****\n",
    "chosing different base learner:\n",
    "1) base learner with high varaince: if we use base leaner with high varaicne such tree with unlimited depth it may overfit the model but bagging can help in reducing varince by combining prediction of different model.\n",
    "2) base leaner with high bias: if we use high bias base learner it may underfit data as it may not able to capture the training dataset complexcity.but essemble technique because of the averaging technique it may result in lowering varaince."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c535013",
   "metadata": {},
   "source": [
    "### ANS 2\n",
    "advantages:\n",
    "1) it reduces overfitting.\n",
    "2) using diverse base leaner leas to a more accurate ensemble model.\n",
    "disadvantages:\n",
    "1) using different model can make esemble model more complex.\n",
    "2) different base learner may have different hyperparameters make it hard for optimization preocess.\n",
    "3) if one model outporm other model then it will be dominated by the average of all the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924aef29",
   "metadata": {},
   "source": [
    "### ANS 5\n",
    "ensemble size is the number of base learner created in bagging. Each base learner training on differnt base sample. ensemble size is the hyperparameter.Large number of base leaner(esseble size) for better accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb30502c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
