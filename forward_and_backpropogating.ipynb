{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7455936b-af47-4b5c-998b-b0b81e108878",
   "metadata": {},
   "source": [
    "### ANS 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad8e285-9267-4855-a209-5b28638ffa2e",
   "metadata": {},
   "source": [
    "Forward propgation in a neural network is process of computing output given a set of input data. \n",
    "1) inputs are fed into neural network \n",
    "2) inputs are passed through layers multiplied by weights and passed through activation function in hidden layer.\n",
    "3) then final activation function is used to give ouput based on weither it is classification problem or regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8255fdc4-fcc6-4830-8f6b-615f1d84ac1b",
   "metadata": {},
   "source": [
    "### ANS 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e900199e-17cb-4e8c-bbf4-7a48ba9a6143",
   "metadata": {},
   "source": [
    "1) we intialize weights w and biases b with random values \n",
    "2) let x be the input vector size n, n is number of input feature. each element of input vecotr corresponds to one feature. \n",
    "3) then we calculate the weighted sum of inputs by its corresponding weight\n",
    "4) then we apply activation function f to the weighted sum to introduce non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e034ee1-be8c-49d0-8fa8-8b5acd6515db",
   "metadata": {},
   "source": [
    "### ANS 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c13338-3c18-4594-91b4-5c5a519594eb",
   "metadata": {},
   "source": [
    "Activation functions are important of neural network.\n",
    "1) weights and baises are combined with inputs.\n",
    "2) then inputs are passed through activation function to introduce non-linearity.\n",
    "3) output of neuron after applying activation fucntion becomes input of next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f2ad9e-86b5-4c9c-911c-f67398e38f61",
   "metadata": {},
   "source": [
    "### ANS 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7576d9b2-7ea4-4b04-88eb-9858997c847e",
   "metadata": {},
   "source": [
    "1) weights are parameter that are associated with connection between neurons in adjacent layers of network.input data is multiplied element-wise with corresponding weights and summed up to produce input of activation function.\n",
    "2) baises are parameters associated with each neuron in network. they represent offset or thershold of activation for each neuron. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f884d709-a97f-461a-91d5-84a1bff43214",
   "metadata": {},
   "source": [
    "### ANS 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3cb9ba-547c-4a81-88c7-1b4cff7778ae",
   "metadata": {},
   "source": [
    "softmax function is commonly used in output layer of a neural network in classification taks where network needs to predict the probabilities of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba1fa84-6900-4059-8720-ee74fa9c0a46",
   "metadata": {},
   "source": [
    "### ANS 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef7394-8c0f-45d0-a7c5-6b3c12ab994b",
   "metadata": {},
   "source": [
    "backpropagation is used to compute the gradient of loss function with respect to weights and biases of network. this gradient information is essential for updating the network parameters during trainig process using optimization algorithims. backpropagation allows network to learn from its mistakes by adjusting parameters in direction to minimize the loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c28d7c4-0a97-4021-a4d0-4fc79ae59f28",
   "metadata": {},
   "source": [
    "### ANS 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e4e518-6da3-4445-8bef-f7f39c2846b3",
   "metadata": {},
   "source": [
    "1) we compute loss gradient. \n",
    "2) then we compute gradient of activation function.\n",
    "3) we calculate gradient with respect to weight\n",
    "4) then we calcuate gradient with respect to baises.\n",
    "5) we update weights and baises using optimization algorithim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c7f59e-4c4a-4c23-8c80-694b0f9b9d3b",
   "metadata": {},
   "source": [
    "### ANS 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f5bf23-3986-48f4-84d9-94611c904efb",
   "metadata": {},
   "source": [
    "chain rule is a fundamental rule which allows us to compute derivates of composite function by breaking it down into simpler parts. \n",
    "1) composite function: in neural network, each layer applies a series of operation to its input data.\n",
    "2) during backward propagation, we will compute gradient of loss function.\n",
    "3) then we apply chain rule iteratively, starting from output layer and propagating backward through the network.\n",
    "4) once we have computed gradient of loss function then we use these gradient to update the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d1dc9e-a78d-4975-b47c-944098016a42",
   "metadata": {},
   "source": [
    "### ANS 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b72e547-95ef-4b5e-84ca-1effe0f7a020",
   "metadata": {},
   "source": [
    "1) Vanishing or Exploding Gradients: in deep networks, gradient can either becomes very small or very large as they progate backward.\n",
    "2) overfitting: overfitting occurs when model learns to memorize training data instead of generalizing to unseen data. this can happen if model is too complex relative to amount of training data.\n",
    "3) choosing inappropriate leraning can lead to slow converge.\n",
    "4) numerical instability can occur leading to overflow or underflow issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244cf151-7400-4299-a4f1-eef48d4fbe1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
