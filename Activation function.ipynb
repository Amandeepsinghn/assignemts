{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3d7b032-93a3-40ba-9a24-a6451ea324a6",
   "metadata": {},
   "source": [
    "# ANS 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8608bef-e972-4fd8-a9b7-db9ee471c589",
   "metadata": {},
   "source": [
    "Activation function helps to determine the output of neural network. Activation function are attached to each neuron, and determine whether it hould be activated or not. Activation function takes weights of each nueron and inputs and bias and multiply or odd to activate the neuron or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79822327-90ab-4f82-987d-5f68a198d2df",
   "metadata": {},
   "source": [
    "# ANS 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfb59df-e880-4550-b32a-0777c5b81771",
   "metadata": {},
   "source": [
    "Commonly used activation function:\n",
    "1) Sigmoid function\n",
    "2) hyperbolic tangent activation function \n",
    "3) RELU\n",
    "4) Leaky RELU\n",
    "5) ELU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb933682-ba05-4b64-b4ad-528873a92575",
   "metadata": {},
   "source": [
    "# ANS 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca38303-296f-4949-995a-584540fe04d7",
   "metadata": {},
   "source": [
    "1) activation function provide non-linear transformation to the data which helps in better understanding of complex pattern.\n",
    "2) non-linearity introduced by activation functions allows for the computation of gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61299ba9-f9f2-426d-ab33-b5fc04658362",
   "metadata": {},
   "source": [
    "# ANS 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e1bbc9-3e7c-470c-8bef-f4c8c60adce5",
   "metadata": {},
   "source": [
    "Sigmoid is a activation function its output is in between 0 and 1.\n",
    "***\n",
    "Advantages:\n",
    "1) smooth gradient,preventing 'jumps' in output values.\n",
    "2) output values bound between 0 and 1\n",
    "3) clear predictions\n",
    "***\n",
    "Disadvantages:\n",
    "1) gradient of the function becomes very small,almost zero.\n",
    "2) it support exponentail operations which is slower for computers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdab082-63ad-45c5-8780-a177a33786dd",
   "metadata": {},
   "source": [
    "# ANS 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2033d3-edcd-437c-8cd0-1879c78734db",
   "metadata": {},
   "source": [
    "Relu is an activation function. It is function which is not fully interval-derivable, but we can take sub-gradient.\n",
    "***\n",
    "Advantages:\n",
    "1) when the input is positive, there is no gradient saturation problem.\n",
    "2) the calculation speed is much faster.it his only linear relationship before and after 0.\n",
    "***\n",
    "Disadvantages:\n",
    "1) when the input is negative, RELU is completly inactive which means that once a negative number is entered relu will not work anymore.\n",
    "***\n",
    "1) Unlike sigmoid relu is not capable when input is below 0.\n",
    "2) Relu compututional is less expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7fb87e-fc98-4f43-96f0-806701c8ba57",
   "metadata": {},
   "source": [
    "# ANS 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0468756-5fe6-4bdb-b7dd-caf9ee8828e8",
   "metadata": {},
   "source": [
    "Advantages of relu function over sigmoid:\n",
    "1) relu does not suffer from saturation problems that affect sigmoid and tanh activation function.\n",
    "2) it is computational efficient.\n",
    "3) it solve the vanishing gradient problem for only positive output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93293e87-7e87-43ff-ab95-40a3829a88b9",
   "metadata": {},
   "source": [
    "# ANS 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ec2cc7-cb3a-453d-ab52-ee365fd8ec4f",
   "metadata": {},
   "source": [
    "Leaky relu is a upgradation of relu. It has the same formula but it uses alpha parameter learning rate which is learned from the back propagation. the mean of output is close to 0 and is zero-centered.\n",
    "***\n",
    "it solves the vanishing gradient problem. for standard relu if the input is zero gradient is zero result in dead neuron but in leaked relu we have some slope for negative input which do not make gradient zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d3153c-4dcc-4ba4-9dc1-0c271f8813df",
   "metadata": {},
   "source": [
    "# ANS 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5600cbd6-6afa-490d-9352-c991d77fbb67",
   "metadata": {},
   "source": [
    "The main purpose of softmax activation function is:\n",
    "1) sigmoid is used for binary classification but softmax is used for multiple classification.\n",
    "2) it gives probability of the multiple classification whichever class has the higher probability that class is selected.\n",
    "3) softmax used for binary classification can be termed as sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e5d303-597b-48b6-9a22-fd4ede440944",
   "metadata": {},
   "source": [
    "# ANS 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dab14b-57b2-4cce-a910-687183a22b21",
   "metadata": {},
   "source": [
    "Tanh is a hyperbolic tangent function.curves of tanh and sigmod function are relatively similar. tanh function has output between -1 and 1.\n",
    "***\n",
    "Difference between tanh and sigmoid\n",
    "1) output range of tanh is 1-and 1 and sigmoid range is 0 and 1 \n",
    "2) derivative of sigmoid function is 0.25 and derivative of tanh function is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c854c93-034a-4ec9-8be5-adfe1cbcbbe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
