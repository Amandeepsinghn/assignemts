{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2bcc27e",
   "metadata": {},
   "source": [
    "### ANS 1\n",
    "KNN algorithim:\n",
    "1) first we have to intialize the k value k>=0.\n",
    "2) then it find the nearest neighbour from the test data. using two distance formula either Euclidian or manhattan.\n",
    "3) for regression it takes average of all nearset point. for classification it the mode value for ouput."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2241821",
   "metadata": {},
   "source": [
    "### ANS 2\n",
    "K value is hyperparamter in KNN. we select the value of KNN by n_neighours hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1202a0",
   "metadata": {},
   "source": [
    "### ANS 3\n",
    "difference between classifier and regressor:\n",
    "1) classifier takes the mode of all nearest point but regressor takes the mean value for the ouput.\n",
    "2) KNN classifier is used for the classififcation statement and KNN regression is used for the regression task.\n",
    "3) classifier is used when we have to predict the class and regression when we have to use continous varaible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f76e880",
   "metadata": {},
   "source": [
    "### ANS 4\n",
    "if we have KNN classification then we can measure the peroformance by using accuracy score but for regression we use r2 scoreto measure the peroframnce."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099c9c90",
   "metadata": {},
   "source": [
    "### ANS 9\n",
    "differnce between Euclidean and Manhattan distance:\n",
    "1) Euclidean distance formula sqrt(X1-X2)**2-(Y1-Y2)**2 and manhattan distance formula is |X1-X2|+|Y1-Y2|.\n",
    "2) Euclidean distance is the shortest path between source and destination but manhattan distance is the sum of all the real distance between source and destination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c50a874",
   "metadata": {},
   "source": [
    "### ANS 5 \n",
    "The curse of dimensionality refers to the challanges and issues that arise when dealing with larger dimensional it affects the KNN algorithtim in which we have calucate the nearest distance point. measuring distance in higher dimesion becomes difficult. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0768ba",
   "metadata": {},
   "source": [
    "### ANS 6\n",
    "we can use handle missing vlaues in many ways:\n",
    "1) We can replace the missing value with the mean,mode or median.\n",
    "2) we can assign 0 to the missing value \n",
    "3) we can find the K nearest neighbour and impute distance based on the non-missing value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caae363",
   "metadata": {},
   "source": [
    "### ANS 7\n",
    "difference between classifier and regressor:\n",
    "1) classifier takes the mode of all nearest point but regressor takes the mean value for the ouput.\n",
    "2) KNN classifier is used for the classififcation statement and KNN regression is used for the regression task.\n",
    "3) classifier is used when we have to predict the class and regression when we have to use continous varaible.\n",
    "*** \n",
    "Classification Perforamce: \n",
    "1) classifier effective for non linear and complex decision boundary \n",
    "2) suffer from curse of dimnesionlaity \n",
    "3) sensitve to irrelevant or redundant feature \n",
    "***\n",
    "Regression performance:\n",
    "1) less sensitive to noises\n",
    "2) can capture non linear relationship in data.\n",
    "3) perform well with smaller number of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1b4a6c",
   "metadata": {},
   "source": [
    "### ANS 8\n",
    "Strengths of KNN algorithm classification and regression:\n",
    "1) classifications: it is simple to implement \n",
    "2) regression: can handle non-linear relationship and can handle complex data.\n",
    "*** \n",
    "Weakness:\n",
    "1) classifcaiton: computational expensive and are sensitve to outliers and irrelevant feature \n",
    "2) regression: sensitve to outliers and noise generally higher memory requirement and computation time.\n",
    "*** \n",
    "Adressing:\n",
    "1) we can solve the curse of dimensinality by using PCA for the feature selection. \n",
    "2) we can solve Outliers problems and noise by doing eda properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16550e1a",
   "metadata": {},
   "source": [
    "### ANS 10\n",
    "In KNN feature scaling plays an crucila role because suppose there are two feature one with range 1 to 100 and one with 1 to 0. feature with larger range will definately dominates the other feature even if it is not correlated with depedent feature so we  scale down the all feature to reomve the domination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa099952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
