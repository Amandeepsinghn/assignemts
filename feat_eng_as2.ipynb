{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e2b7728-b83b-479d-8cdf-5d2b456e7cd9",
   "metadata": {},
   "source": [
    "### ANS 1\n",
    "min max scalling is used to scale the data between 0 and 1 it is mainly used for deep learning model. for example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ccefd99-8a06-40c1-9529-a01bea420010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b8b3c92-53a8-4f60-9556-74e7df2b43c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68204d3f-63c1-4bdd-a841-55a04ac53724",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd312ae8-2a6a-4e79-98bd-ae732859bf70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9b48fa6-77b7-4e97-b2f4-453202cfe81a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d03db07-7ea5-417a-9104-7da5bf9ae677",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s=scaler.fit_transform(df[['sepal_length','sepal_width']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d489aeed-6dd5-4908-a9e6-b0ea62342c82",
   "metadata": {},
   "source": [
    "### ANS 2\n",
    "unit vector technique is also used for scalling it convert a specific vecotr to a unit by dividing the vector by it maginitude for example:\n",
    "x=(3,4)\n",
    "|x|=sqrt(3/5,4/5)\n",
    "|x|=1\n",
    "****\n",
    "min max and unit vector both scale the data to a specific range "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4ff78f-902d-4e1d-a643-0f125d448009",
   "metadata": {},
   "source": [
    "### ANS 3\n",
    "pca stands for principle component analysis it is used to find the most important data between various dimensions of features. it used as dimensionally reduction because pcm find orthogonal axes called principal component which capture the maximum variance in data for example take two feature car_price and car_size pcm will find pricipal component that combines height and weight into a single variable thus reduce the dimensionality from two to one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7823fd6a-5c1f-4128-828a-7d8039b26bd2",
   "metadata": {},
   "source": [
    "### ANS 5\n",
    "steps:\n",
    "1) we will make the dataframe \n",
    "2) import MinMax\n",
    "3) declare any varaible like scale=MinMaxscaler \n",
    "4) use scaler.fit_trasform function to scale the data between 1 and 0\n",
    "5) now we can use the data for conclusion like making graph etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b5bb47-f2d7-4569-9495-1f0446e697a5",
   "metadata": {},
   "source": [
    "### ANS 6\n",
    "STEPS:\n",
    "1) first make dataframe \n",
    "2) either we can select three features or two feature\n",
    "3) lets suppose we select two feature we will use pcm to find pricipal component which cover all the varaince of data as possible which the dimensionality to one.\n",
    "4) we can do this process as many times as we want like if there are 100 feature and we want to make one important feature out of all of the features then we can use pcm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc68d678-b859-472e-85ac-ceca9706fb7c",
   "metadata": {},
   "source": [
    "### ANS 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a0dff9b-480b-402a-9944-851e83a66149",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l=[1,5,10,15,20]\n",
    "df=pd.DataFrame(data=l,columns=['random'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28a00fbb-fa98-4093-ad50-2d181f1d911c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "419138b1-5356-463b-96bc-d2e159956b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler(feature_range=(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc42fa27-35c4-407f-a4e0-705991377b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ],\n",
       "       [-0.57894737],\n",
       "       [-0.05263158],\n",
       "       [ 0.47368421],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit_transform(df[['random']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ad4877-aecb-4d7b-89f0-9c0f220c2eed",
   "metadata": {},
   "source": [
    "### ANS 8\n",
    "lets suppose we get 5 pcm:\n",
    "1) pc1= 60%\n",
    "2) pc2=20%\n",
    "3) pc3=10%\n",
    "4) pc4=6%\n",
    "5) pc5=4%\n",
    "***\n",
    "lets suppose our thershold is 90% and by adding pc1,pc2,pc3 we get 90% of the thershold so we select pc1,pc2 and pc3 \n",
    "***\n",
    "keeping too many component can lead to high dimensional dataset with low reduction in complexity and keeping few component can lead to loss of critical information\n",
    "*** \n",
    "1) pc1 capture the dominat pattern which covers the 60% of the vaiation of data.\n",
    "2) PC2 is also a linear combination of orgianl feautre but is orthogonal to pc1.pc2 capture the second dominant pattern which cover 20% the varaiation of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebae7524-baa4-435b-be13-892114b45e45",
   "metadata": {},
   "source": [
    "### ANS  4\n",
    "pca is used for the feature extraction it is used for the dimension reduction which select the principle component that contains the largest variation of data that helps in selection feature slection. for example if have pc1 and pc2 which sum up to contain 80% varaition of the data so we can use these two feature for the conclusion of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd1ad8d-04a2-48a6-ac4f-b89d269700c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
