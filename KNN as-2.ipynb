{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddef512c",
   "metadata": {},
   "source": [
    "### ANS 1\n",
    "main difference between Euclidean and Manhattan:\n",
    "1) Euclidean distance formula sqrt(X1-X2)**2-(Y1-Y2)**2 and manhattan distance formula is |X1-X2|+|Y1-Y2|.\n",
    "2) Euclidean distance is the shortest path between source and destination but manhattan distance is the sum of all the real distance between source and destination.\n",
    "***\n",
    "perforamnce of KNN classifier or regressor:\n",
    "1) sensitive to oulier: manhattan distance is oftem more robust to outliers as it sums the distance between difference but euclidean distance takes the square of differences\n",
    "2) feature scaling: Euclidean distance can be sensitve to scale feature , meaning if some feature have larger scale than other they might dominate the distance calculation. but manhattan is less affected by scales. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d55ae5",
   "metadata": {},
   "source": [
    "### ANS 2\n",
    "we can find the optimal value of K by using grid search or Randomized search cv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed66821",
   "metadata": {},
   "source": [
    "### ANS 3\n",
    "\n",
    "1) Manhattan distance:when our model have so many outliers than we can choose manhattan distance and if there is great range between feature range then we can also use manhattan distance. \n",
    "2) Euclidean distance: when the feature are measured in the same unit and not affected by the outliers then we can use eulcidean distance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e00a7",
   "metadata": {},
   "source": [
    "### ANS 4\n",
    "Some common parameter are:\n",
    "1) n_neighbors= it used to give the number of k.\n",
    "2) algorithim= it allows us to which algorithim to use.\n",
    "3) p= it comprised of two value 1 and 0. 1 is used for manhattan and other for euclidean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cb88a3",
   "metadata": {},
   "source": [
    "### ANS 5\n",
    "Small training dataset:\n",
    "1) it may led to overfitting \n",
    "2) it can result in high varaince \n",
    "***\n",
    "large dataset:\n",
    "1) A large dataset may lead to better generalization it helps model to understand the underlying pattern in the data.\n",
    "2) it can reduce the overfitting of the model.\n",
    "***\n",
    "technique to optimize:\n",
    "1) Cross validation:\n",
    "it allows asses the model's performance across different trainning dataset.\n",
    "2) feature selection: \n",
    "choose the most important feature to reduce the dimesionality and train the data on less dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6876312",
   "metadata": {},
   "source": [
    "### ANS 6\n",
    "drawbacks of KNN classifier or regression:\n",
    "1) Computationallly expensive.\n",
    "2) sensitive to outliers and noise.\n",
    "3) Curse of dimensionality.\n",
    "4) Irellevant feature: feature which are not relevant to the prediction can impact model peformance as kNN consider all feature equal.\n",
    "***\n",
    "we can solve this probelm by feature selection,outlier detection, dimensionality reduction and feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8f9cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
